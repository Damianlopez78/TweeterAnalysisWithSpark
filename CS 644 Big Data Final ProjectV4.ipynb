{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "marked-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "import sys\n",
    "import requests\n",
    "from collections import namedtuple\n",
    "from pyspark import SparkConf\n",
    "\n",
    "\n",
    "def process_stream(record, spark):\n",
    "    if not record.isEmpty():\n",
    "        df = spark.createDataFrame(record) \n",
    "        df.show()\n",
    "        \n",
    "def aggregate_tags_count(new_values, total_sum):\n",
    "    return sum(new_values) + (total_sum or 0)\n",
    "\n",
    "def get_sql_context_instance(spark_context):\n",
    "    if ('sqlContextSingletonInstance' not in globals()):\n",
    "            globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n",
    "    return globals()['sqlContextSingletonInstance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rdd(time, rdd):\n",
    "    print(\"----------- %s -----------\" % str(time))\n",
    "    try:\n",
    "       # Get spark sql singleton context from the current context\n",
    "        sql_context = get_sql_context_instance(rdd.context)\n",
    "        #print(\"Get spark sql singleton context from the current context ----------- %s -----------\" % str(time))\n",
    "        #sqlContext = SQLContext(sc)\n",
    "        rows_rdd = rdd.map(lambda x: Row(tweet_id=x[0], tweet_screen_name=x[1], tweet_text=[2], source=x[3], device=x[4], tweet_country_code=x[5], tweet_createdat=x[6]))\n",
    "\n",
    "        # create a DF from the Row RDD\n",
    "        test_df = sql_context.createDataFrame(rows_rdd)\n",
    "        test_df.show()\n",
    "        # Register the dataframe as table\n",
    "        test_df.registerTempTable(\"tweets\")\n",
    "        #getSQLContext = get_sql_context_instancet(rdd.context)\n",
    "        \n",
    "        \n",
    "        # convert the RDD to Row RDD\n",
    "        #row_rdd = rdd.map(lambda w: Row(word=w[0], word_count=w[1]))\n",
    "        #map it\n",
    "        #rows_rdd = rdd.map(lambda x: Row(tweet_id=x[0], tweet_screen_name=x[1], tweet_text=[2], source=x[3], device=x[4], tweet_country_code=x[5], tweet_createdat=x[6]))\n",
    "        \n",
    "        \n",
    "        #rows_rdd = rdd.map(lambda x: Row(tweet_id=x[0], tweet_screen_name=x[1], tweet_text=[2], source=x[3], device=x[4], tweet_country_code=x[5], tweet_createdat=x[6]))\n",
    "        \n",
    "        #rows_rdd.foreachRDD(lambda rdd: process_stream(rdd, spark))\n",
    "\n",
    "   \n",
    "\n",
    "   \n",
    "        # get the top 10 test from the table using SQL and print them\n",
    "        # get data into a dataframe\n",
    "        \n",
    "        testContextDF = sql_context.sql(\"select tweet_id , tweet_screen_name, tweet_text, source, device, tweet_country_code, tweet_createdat from tweets order by tweet_id desc limit 200\")\n",
    "        testContextDF.coalesce(1).write.format(('com.databricks.spark.csv').mode('overwrite').option(\"header\", \"true\").csv(\"testData.csv\") )\n",
    "\n",
    "        #return testContext\n",
    "        #new_testDF = testContext.toPandas()\n",
    "        #test_df.head()\n",
    "        #hashtag_counts_df.coalesce(1).write.format('com.databricks.spark.csv').mode('overwrite').option(\"header\", \"true\").csv(\"/Users/girishdurgaiah/hashtag_file.csv\") \n",
    "   \n",
    "        #country_counts_df = sql_context.sql(\"select word as country_code, word_count as tweet_count from test where word like 'CC%'order by word_count desc limit 10\")\n",
    "        #country_counts_df.show()\n",
    "        #country_counts_df.coalesce(1).write.format('com.databricks.spark.csv').mode('overwrite').option(\"header\", \"true\").csv(\"/Users/girishdurgaiah/country_file.csv\")\n",
    "   \n",
    "        #device_df = sql_context.sql(\"select word as device, word_count as device_count from test where word like 'TS%'order by word_count desc limit 10\")\n",
    "        #device_df.show()\n",
    "        #device_df.coalesce(1).write.format('com.databricks.spark.csv').mode('overwrite').option(\"header\", \"true\").csv(\"/Users/girishdurgaiah/device_file.csv\")\n",
    "           \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# create spark configuration\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"TwitterStreamApp\")\n",
    "\n",
    "# create spark context with the above configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# create the Streaming Context from the above spark context with interval size 2 seconds\n",
    "ssc = StreamingContext(sc, 2)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "# setting a checkpoint to allow RDD recovery\n",
    "ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "\n",
    "# read data from port 5555\n",
    "dataStream = ssc.socketTextStream(\"localhost\",5550)\n",
    "dataStream.foreachRDD(process_rdd)\n",
    "\n",
    "#lines = dataStream.flatMap(lambda text: text.split(\" \"))\n",
    "#lines = dataStream.window(20)\n",
    "#fields = (\"tweet_id\" , \"tweet_screen_name\", \"tweet_text\", \"source\", \"device\", \"tweet_country_code\", \"tweet_createdat\")\n",
    "#(lines.flatMap(lambda rec: Tweet( rec[0], rec[1], rec[2], rec[3],rec[4], rec[5], rec[6])).map(lambda rec: Tweet(rec[0], rec[1], rec[2], rec[3], rec[4], rec[5], rec[6])).foreachRDD(lambda rdd: rdd.toDF().limit(200).registerTempTable(\"tweets\")))\n",
    "\n",
    "\n",
    "# split each tweet into words\n",
    "#words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "              \n",
    "# filter the words to get only test, then map each hashtag to be a pair of (hashtag,1)\n",
    "#test = words.map(lambda x: (x, 1)) \n",
    "\n",
    "# adding the count of each hashtag to its last count\n",
    "#tags_totals = test.updateStateByKey(aggregate_tags_count)\n",
    "\n",
    "# do processing for each RDD generated in each interval\n",
    "#tags_totals.foreachRDD(process_rdd)\n",
    "#lines.foreachRDD(lambda rdd: rdd.toDF())\n",
    "#lines.map( lambda rec: Tweet( rec[0], rec[1], rec[2], rec[3],rec[4], rec[5], rec[6]))#.foreachRDD(lambda rdd: rdd.toDF())\n",
    "\n",
    "\n",
    "# start the streaming computation\n",
    "ssc.start()\n",
    "\n",
    "# wait for the streaming to finish\n",
    "#ssc.awaitTermination()\n",
    "\n",
    "#end\n",
    "#scc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "top200_tweets = sqlContext.sql('select tweet_id, tweet_screen_name, tweet_text, source, device, tweet_country_code, tweet_createdat from tweets order by tweet_id limit 200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "testContextDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for the streaming to finish\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "testContextDF = sqlContext.sql(\"select tweet_id , tweet_screen_name, tweet_text, source, device, tweet_country_code, tweet_createdat from tweets order by tweet_id desc limit 200\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "confidential-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to compute sentiments of the received tweets\n",
    "def get_prediction(tweet_text):\n",
    "    try:\n",
    "    # filter the tweets whose length is greater than 0\n",
    "        rdd = tweet_text.filter(lambda x: len(x) > 0)\n",
    "    # create a dataframe with column name 'tweet' and each row will contain the tweet\n",
    "        rows_rdd = rdd.map(lambda x: Row(tweet_id=x[0], tweet_screen_name=x[1], tweet_text=[2], source=x[3], device=x[4], tweet_country_code=x[5], tweet_createdat=x[6]))\n",
    "    # create a spark dataframe\n",
    "        wordsDataFrame = spark.createDataFrame(rows_rdd)\n",
    "    # create a table\n",
    "        wordsDataFrame.registerTable(\"tweets_table\")\n",
    "        testDF = SQLContext.sql(\"select tweet_id, tweet_screen_name, tweet_text, source, device, tweet_country_code, tweet_createdat from tweets_table order by tweet_id limit 200\")\n",
    "    \n",
    "    except : \n",
    "        print('No data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reflected-aging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n",
      "No data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1dffdb4ed7f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeachRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.2_4/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create spark configuration\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"checkpoint_TwitterApp\")\n",
    "\n",
    "# create spark context with the above configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "#Streamin contest\n",
    "ssc = StreamingContext(sc, batchDuration=3)\n",
    "lines = ssc.socketTextStream(\"localhost\", 5551)\n",
    "words = lines.flatMap(lambda x: x.split(\"\"))\n",
    "words.foreachRDD(get_prediction)\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-budget",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
